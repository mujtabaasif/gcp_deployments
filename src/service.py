"""FastAPI service exposing the ConvFinQA agent."""

from __future__ import annotations

import logging
import os
from functools import lru_cache
from typing import Any, Dict, List

from fastapi import FastAPI, HTTPException
from langchain_openai import ChatOpenAI
from langgraph.graph import START
from pydantic import BaseModel, Field

from .agent import FinancialReasoningAgent
from .dataloader import ConvFinQARecord, State, load_convfinqa_dataset
from .prompts import PromptBuilder
from .utils import get_context


logger = logging.getLogger(__name__)

MODEL_ID = os.getenv("MODEL_ID", "Qwen/Qwen3-8B")
BASE_URL = os.getenv("OPENAI_BASE_URL", "http://localhost:8000/v1")
API_KEY = os.getenv("OPENAI_API_KEY", "API_KEY")
DATA_PATH = os.getenv("DATA_PATH", "data/convfinqa_dataset.json")


class ChatTurn(BaseModel):
    """Represents a single conversation turn."""

    user: str = Field(description="The user question for this turn")
    assistant: str = Field(description="The assistant response for this turn")


class ChatRequest(BaseModel):
    """Request payload for the /chat endpoint."""

    record_id: str = Field(description="Identifier of the ConvFinQA record to query")
    question: str = Field(description="The user question to process")
    history: List[ChatTurn] = Field(
        default_factory=list,
        description="Prior conversation history for additional context",
    )


class ChatResponse(BaseModel):
    """Response payload for the /chat endpoint."""

    answer: str = Field(description="Formatted answer generated by the agent")
    history: List[ChatTurn] = Field(
        description="Updated conversation history including the latest answer",
    )


class RecordResponse(BaseModel):
    """Minimal representation of a ConvFinQA record."""

    id: str
    pre_text: str
    post_text: str
    table: Dict[str, Dict[str, object]]


def _load_llm() -> ChatOpenAI:
    """Initialise the language model client."""

    return ChatOpenAI(
        base_url=BASE_URL,
        api_key=API_KEY,
        model=MODEL_ID,
    )


@lru_cache(maxsize=1)
def _load_records() -> tuple[Dict[str, ConvFinQARecord], List[ConvFinQARecord]]:
    """Load ConvFinQA records from disk once per process."""

    train_records, test_records = load_convfinqa_dataset(DATA_PATH)
    index = {record.id: record for record in (*train_records, *test_records)}
    if not index:
        raise RuntimeError("No ConvFinQA records were loaded; check DATA_PATH")
    return index, train_records


@lru_cache(maxsize=1)
def _build_agent() -> tuple[FinancialReasoningAgent, Dict[str, ConvFinQARecord]]:
    """Instantiate the ConvFinQA reasoning agent and supporting artefacts."""

    records_index, train_records = _load_records()
    few_shot_examples = train_records[:3] if train_records else []
    prompt_builder = PromptBuilder(few_shot_examples)
    agent = FinancialReasoningAgent(_load_llm(), prompt_builder)
    return agent, records_index


@lru_cache(maxsize=1)
def _get_runtime() -> tuple[Any, Dict[str, ConvFinQARecord]]:
    """Return the compiled agent graph and record index."""

    agent, records_index = _build_agent()
    return agent.build_graph(), records_index


app = FastAPI(
    title="ConvFinQA Agent Service",
    version="0.1.0",
    summary="FastAPI service wrapper for the ConvFinQA reasoning agent",
)


@app.get("/health")
def health() -> Dict[str, str]:
    """Light-weight readiness probe."""

    try:
        _get_runtime()
    except Exception as exc:  # pragma: no cover - defensive logging
        logger.exception("Service health check failed: %s", exc)
        raise HTTPException(status_code=500, detail="service unavailable") from exc
    return {"status": "ok"}


@app.get("/records", response_model=List[str])
def list_records() -> List[str]:
    """Return the identifiers of all available ConvFinQA records."""

    _, records_index = _get_runtime()
    return list(records_index.keys())


@app.get("/records/{record_id}", response_model=RecordResponse)
def get_record(record_id: str) -> RecordResponse:
    """Fetch the source material for a specific ConvFinQA record."""

    _, records_index = _get_runtime()
    try:
        record = records_index[record_id]
    except KeyError as exc:
        raise HTTPException(status_code=404, detail="record not found") from exc
    return RecordResponse(
        id=record.id,
        pre_text=record.doc.pre_text,
        post_text=record.doc.post_text,
        table=record.doc.table,
    )


@app.post("/chat", response_model=ChatResponse)
def chat(request: ChatRequest) -> ChatResponse:
    """Run a full ConvFinQA reasoning cycle for the requested question."""

    graph, records_index = _get_runtime()

    try:
        record = records_index[request.record_id]
    except KeyError as exc:
        raise HTTPException(status_code=404, detail="record not found") from exc

    context = get_context(record)
    history_payload = [turn.model_dump() for turn in request.history]
    state = State(
        question=request.question,
        context=context,
        retriever="",
        steps_generated="",
        executor="",
        answer="",
        retrieval_history=[],
        generation_history=[],
        history=history_payload,
        skip_generation=False,
    )

    state_dict = graph.invoke(state, start=START)
    next_state = State(**state_dict)

    enriched_history = [ChatTurn(**turn) for turn in next_state.history]
    return ChatResponse(answer=next_state.answer.strip(), history=enriched_history)
